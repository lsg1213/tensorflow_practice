{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice2. Softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as pickle\n",
    "import time\n",
    "\n",
    "# set default plot options\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_CIFAR10_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr, X_val, Y_val, X_te, Y_te, mean_img = get_CIFAR10_data()\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape))\n",
    "print ('Val data shape : %s,  Val labels shape : %s' % (X_val.shape, Y_val.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*10000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_tr[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_tr[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement two version of loss functions for softmax classifier, and test it out on the CIFAR10 dataset.\n",
    "\n",
    "First, implement the naive softmax loss function with nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax_loss(Weights,X_data,Y_data,reg):\n",
    "    \"\"\"\n",
    "     Inputs have D dimension, there are C classes, and we operate on minibatches of N examples.\n",
    "    \n",
    "     Inputs :\n",
    "         - Weights : A numpy array of shape (D,C) containing weights.\n",
    "         - X_data : A numpy array of shape (N,D) contatining a minibatch of data.\n",
    "         - Y_data : A numpy array of shape (N,) containing training labels; \n",
    "               Y[i]=c means that X[i] has label c, where 0<=c<C.\n",
    "         - reg : Regularization strength. (float)\n",
    "         \n",
    "     Returns :\n",
    "         - loss as single float\n",
    "         - gradient with respect to Weights; an array of sample shape as Weights\n",
    "     \"\"\"\n",
    "    \n",
    "    # Initialize the loss and gradient to zero\n",
    "    softmax_loss = 0.0\n",
    "    dWeights = np.zeros_like(Weights)\n",
    "    \n",
    "    ####################################################################################################\n",
    "    # TODO : Compute the softmax loss and its gradient using explicit loops.                           # \n",
    "    #        Store the loss in loss and the gradient in dW.                                            #\n",
    "    #        If you are not careful here, it is easy to run into numeric instability.                  #\n",
    "    #        Don't forget the regularization.                                                          #\n",
    "    #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "    #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "    ####################################################################################################\n",
    "    return softmax_loss, dWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random softmax weight matrix and use it to compute the loss. As a rough sanity check, our loss should be something close to -log(0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = naive_softmax_loss(W, X_tr, Y_tr, 0.0)\n",
    "\n",
    "print ('loss :', loss)\n",
    "print ('sanity check : ', -np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is the vectorized softmax loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_softmax_loss(Weights, X_data, Y_data, reg):\n",
    "    softmax_loss = 0.0\n",
    "    dWeights = np.zeros_like(Weights)\n",
    "\n",
    "    ####################################################################################################\n",
    "    # TODO : Compute the softmax loss and its gradient using no explicit loops.                        # \n",
    "    #        Store the loss in loss and the gradient in dW.                                            #\n",
    "    #        If you are not careful here, it is easy to run into numeric instability.                  #\n",
    "    #        Don't forget the regularization.                                                          #\n",
    "    #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "    #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "    ####################################################################################################\n",
    "    \n",
    "    return softmax_loss, dWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare two versions. The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "loss_naive, grad_naive = naive_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('naive loss : %e with %fs' % (loss_naive, time.time()-s_time))\n",
    "\n",
    "s_time = time.time()\n",
    "loss_vectorized, grad_vectorized = vectorized_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('vectorized loss : %e with %fs' % (loss_vectorized, time.time()-s_time))\n",
    "\n",
    "print ('loss difference : %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print ('gradient difference : %f' % np.linalg.norm(grad_naive-grad_vectorized, ord='fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should implement the softmax classifier using the comment below with softmax loss function you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    def __init__(self):\n",
    "        #self.Weights = None\n",
    "        return\n",
    "        \n",
    "    def train(self, X_tr_data, Y_tr_data, X_val_data, Y_val_data, lr=1e-3, reg=1e-5, iterations=100, bs=128, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this Softmax classifier using stochastic gradient descent.\n",
    "        \n",
    "        Inputs have D dimensions, and we operate on N examples.\n",
    "        \n",
    "        Inputs :\n",
    "            - X_data : A numpy array of shape (N,D) containing training data.\n",
    "            - Y_data : A numpy array of shape (N,) containing training labels;\n",
    "                  Y[i]=c means that X[i] has label 0<=c<C for C classes.\n",
    "            - lr : (float) Learning rate for optimization.\n",
    "            - reg : (float) Regularization strength. \n",
    "            - iterations : (integer) Number of steps to take when optimizing. \n",
    "            - bs : (integer) Number of training examples to use at each step.\n",
    "            - verbose : (boolean) If true, print progress during optimization.\n",
    "        \n",
    "        Regurns :\n",
    "            - A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_train, dim = X_tr_data.shape\n",
    "        num_classes = np.max(Y_tr_data)+1\n",
    "        self.Weights = 0.001*np.random.randn(dim, num_classes)\n",
    "            \n",
    "        for it in range(iterations):\n",
    "            #X_batch = None\n",
    "            #Y_batch = None\n",
    "            \n",
    "            ####################################################################################################\n",
    "            # TODO : Sample batch_size elements from the training data and their corresponding labels          #\n",
    "            #        to use in this round of gradient descent.                                                 #\n",
    "            #        Store the data in X_batch and their corresponding labels in Y_batch; After sampling       #\n",
    "            #        X_batch should have shape (batch_size, dim) and Y_batch should have shape (batch_siae,)   #\n",
    "            #                                                                                                  #\n",
    "            #        Hint : Use np.random.choice to generate indicies.                                         #\n",
    "            #               Sampling with replacement is faster than sampling without replacement.             #\n",
    "            #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "            #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "            ####################################################################################################\n",
    "\n",
    "            # Evaluate loss and gradient\n",
    "            tr_loss, tr_grad = self.loss(X_batch, Y_batch, reg)\n",
    "\n",
    "            # Perform parameter update\n",
    "            ####################################################################################################\n",
    "            # TODO : Update the weights using the gradient and the learning rate                               #\n",
    "            #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\\\n",
    "            #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "            ####################################################################################################\n",
    "\n",
    "            if verbose and it % num_iters == 0:\n",
    "                print ('Ieration %d / %d : loss %f ' % (it, num_iters, loss))\n",
    "            \n",
    "        \n",
    "    \n",
    "    def predict(self, X_data):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this softmax classifier to predict labels for data points.\n",
    "        \n",
    "        Inputs :\n",
    "            - X : A numpy array of shape (N,D) containing training data.\n",
    "            \n",
    "        Returns :\n",
    "             - Y_pred : Predicted labels for the data in X. Y_pred is a 1-dimensional array of length N, \n",
    "                        and each element is an integer giving the predicted class.\n",
    "        \"\"\"\n",
    "        Y_pred = np.zeros(X_data.shape[0])\n",
    "        \n",
    "        ####################################################################################################\n",
    "        # TODO : Implement this method. Store the predicted labels in Y_pred                               #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        return Y_pred\n",
    "    \n",
    "    def get_accuracy(self, X_data, Y_data):\n",
    "        \"\"\"\n",
    "        Use X_data and Y_data to get an accuracy of the model.\n",
    "        \n",
    "        Inputs :\n",
    "            - X_data : A numpy array of shape (N,D) containing input data.\n",
    "            - Y_data : A numpy array of shape (N,) containing a true label.\n",
    "            \n",
    "        Returns :\n",
    "             - Accuracy : Accuracy of input data pair [X_data, Y_data].\n",
    "        \"\"\"\n",
    "        ####################################################################################################\n",
    "        # TODO : Implement this method. Calculate an accuracy of X_data using Y_data and predict Func                               #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def loss(self, X_batch, Y_batch, reg):\n",
    "        return vectorized_softmax_loss(self.Weights, X_batch, Y_batch, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the validatoin set to tune hyperparemeters (regularizatoin strength and learning rate).\n",
    "You should experiment with different range for the learning rates and regularization strength;\n",
    "if you are careful you should be able to get a classification accuracy of over 0.35 on the validatoin set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results is dictionary mapping tuples of the form.\n",
    "# (learning_rate, regularization_strength) to tuple of the form (training_accuracy, validation_accuracy).\n",
    "# The accuracy is simply the fraction of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-8, 1e-7, 5e-7, 1e-6]\n",
    "regularization_strengths = [5e2, 1e3, 1e4, 5e4]\n",
    "\n",
    "#########################################################################################################\n",
    "# TODO : Write code that chooses the best hyperparameters by tuning on the validation set.              # \n",
    "#        For each combination of hyperparemeters, train a Softmax on the training set,                  #\n",
    "#        compute its accuracy on the training and validatoin sets, and store these numbers in the       #\n",
    "#        results dictionary. In addition, store the best validation accuracy in best_val                #\n",
    "#        and the Softmax object that achieves this accuracy in best_softmax.                            #\n",
    "#                                                                                                       #\n",
    "# Hint : You should use a small value for num_iters as you develop your validation code so that the     #\n",
    "#        Softmax don't take much time to train; once you are confident that your validation code works, #\n",
    "#        you should rerun the validation code with a larger value for num_iter.                         #\n",
    "#------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "#softmax = Softmax()\n",
    "\n",
    "#-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "#########################################################################################################\n",
    "\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print ('lr %e reg %e train accuracy : %f, val accuracy : %f ' % (lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validatoin accuracy achieved during cross-validation :', best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best softmax on testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_te_pred = best_softmax.predict(X_te)\n",
    "test_accuracy = np.mean(Y_te == Y_te_pred)\n",
    "\n",
    "print ('softmax on raw pixels final test set accuracy : ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize (Image, Predicted label) pairs of the best softmax model. Results may are not good because we train simple softmax classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*1000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_te[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_te_pred[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learned weights for each class. Depending on your choice of learning rate and regularization strength, these may or may not be nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_softmax.Weights[:-1, :]\n",
    "w = w.reshape(32,32,3,10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    \n",
    "    wimg=255.0*(w[:,:,:,i].squeeze() - w_min)/(w_max-w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
